<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BrainSeg AI - Neural Segmentation Research</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <style>
        :root {
            --primary: #6B46C1;
            --primary-dark: #553C9A;
            --dark: #111111;
            --darker: #000000;
            --light: #F9FAFB;
            --gray: #4A5568;
            --gray-light: #A0AEC0;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: var(--light);
            background-color: var(--dark);
        }
        
        .container {
            width: 90%;
            max-width: 1200px;
            margin: 0 auto;
            padding: 1rem;
        }
        
        header {
            background-color: var(--darker);
            padding: 3rem 0;
            position: relative;
            overflow: hidden;
        }
        
        header::after {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, var(--primary-dark) 0%, transparent 70%);
            opacity: 0.4;
            z-index: 1;
        }
        
        header .container {
            position: relative;
            z-index: 2;
        }
        
        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1.5rem 0;
        }
        
        .logo {
            font-size: 1.5rem;
            font-weight: 700;
            display: flex;
            align-items: center;
        }
        
        .logo span {
            color: var(--primary);
        }
        
        .nav-links {
            display: flex;
            list-style: none;
        }
        
        .nav-links li {
            margin-left: 2rem;
        }
        
        .nav-links a {
            color: var(--light);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        
        .nav-links a:hover {
            color: var(--primary);
        }
        
        .hero {
            padding: 4rem 0;
            text-align: center;
        }
        
        #brain-container {
            width: 250px;
            height: 250px;
            margin: 0 auto 2rem auto;
            position: relative;
        }
        
        #brain-model {
            width: 100%;
            height: 100%;
        }
        
        h1 {
            font-size: 2.6rem;
            line-height: 1.2;
            margin-bottom: 1.5rem;
            background: linear-gradient(90deg, #fff, var(--primary));
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
        }
        
        .tagline {
            font-size: 1.2rem;
            color: var(--gray-light);
            margin-bottom: 2rem;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }
        
        .btn {
            display: inline-block;
            background-color: var(--primary);
            color: white;
            padding: 0.75rem 1.75rem;
            text-decoration: none;
            border-radius: 4px;
            font-weight: 600;
            transition: all 0.3s ease;
            border: none;
            cursor: pointer;
        }
        
        .btn:hover {
            background-color: var(--primary-dark);
            transform: translateY(-2px);
            box-shadow: 0 10px 15px rgba(85, 60, 154, 0.2);
        }
        
        section {
            padding: 5rem 0;
        }
        
        section:nth-child(even) {
            background-color: var(--darker);
        }
        
        h2 {
            font-size: 2rem;
            margin-bottom: 2rem;
            position: relative;
            display: inline-block;
            color: var(--light);
        }
        
        h2::after {
            content: "";
            position: absolute;
            left: 0;
            bottom: -10px;
            height: 4px;
            width: 60px;
            background-color: var(--primary);
        }
        
        .results-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 3rem;
            margin-top: 3rem;
        }
        
        .result-img {
            width: 100%;
            border-radius: 8px;
            transition: all 0.3s ease;
        }
        
        .result-img:hover {
            transform: scale(1.02);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.4);
        }
        
        .stats {
            margin-top: 3rem;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 2rem;
            text-align: center;
        }
        
        .stat-card {
            background-color: rgba(0, 0, 0, 0.2);
            padding: 2rem;
            border-radius: 8px;
            transition: all 0.3s ease;
            border: 1px solid rgba(107, 70, 193, 0.2);
        }
        
        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.3);
            border-color: var(--primary);
        }
        
        .stat-value {
            font-size: 2.5rem;
            font-weight: bold;
            color: var(--primary);
            margin-bottom: 0.5rem;
        }
        
        .stat-label {
            color: var(--gray-light);
            font-size: 1rem;
        }
        
        .faq {
            max-width: 800px;
            margin: 3rem auto 0;
        }
        
        .faq-category {
            margin-bottom: 3rem;
        }
        
        .faq-category h3 {
            color: var(--primary);
            font-size: 1.6rem;
            margin-bottom: 1.5rem;
            border-bottom: 2px solid var(--primary-dark);
            padding-bottom: 0.5rem;
            display: inline-block;
        }
        
        .faq-item {
            margin-bottom: 1.5rem;
            border-bottom: 1px solid rgba(107, 70, 193, 0.2);
            padding-bottom: 1.5rem;
        }
        
        .faq-question {
            font-size: 1.2rem;
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: var(--light);
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .faq-question:hover {
            color: var(--primary);
        }
        
        .faq-answer {
            color: var(--gray-light);
        }
        
        footer {
            background-color: var(--darker);
            padding: 3rem 0;
            text-align: center;
            position: relative;
        }
        
        footer::before {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 5px;
            background: linear-gradient(90deg, var(--primary-dark), var(--primary), var(--primary-dark));
        }
        
        .contact-info {
            margin-top: 1.5rem;
            color: var(--gray-light);
        }
        
        .contact-info a {
            color: var(--primary);
            text-decoration: none;
            transition: color 0.3s ease;
        }
        
        .contact-info a:hover {
            color: var(--light);
        }
        
        .team-members {
            font-style: italic;
            color: var(--gray-light);
            margin-top: 1rem;
        }
        
        /* Ratings and Feedback Section Styles */
        .rating-models {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 3rem 0;
        }
        
        .model-card {
            background-color: rgba(0, 0, 0, 0.2);
            border-radius: 8px;
            padding: 2rem;
            border: 1px solid rgba(107, 70, 193, 0.2);
            transition: all 0.3s ease;
        }
        
        .model-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.3);
            border-color: var(--primary);
        }
        
        .model-name {
            font-size: 1.4rem;
            font-weight: 600;
            color: var(--primary);
            margin-bottom: 1rem;
            text-align: center;
        }
        
        .model-description {
            color: var(--gray-light);
            margin-bottom: 1.5rem;
        }
        
        .star-rating {
            display: flex;
            justify-content: center;
            margin-bottom: 1.5rem;
        }
        
        .star {
            color: #A0AEC0; /* Default gray color */
            font-size: 2rem;
            cursor: pointer;
            transition: transform 0.2s ease;
            margin: 0 0.2rem;
        }
        
        .star:hover {
            transform: scale(1.2);
        }
        
        .comment-form {
            background-color: rgba(0, 0, 0, 0.2);
            border-radius: 8px;
            padding: 2rem;
            max-width: 800px;
            margin: 3rem auto 0;
            border: 1px solid rgba(107, 70, 193, 0.2);
        }
        
        .form-group {
            margin-bottom: 1.5rem;
        }
        
        .form-label {
            display: block;
            margin-bottom: 0.5rem;
            color: var(--light);
        }
        
        .form-control {
            width: 100%;
            padding: 0.75rem;
            border-radius: 4px;
            border: 1px solid rgba(107, 70, 193, 0.2);
            background-color: rgba(0, 0, 0, 0.1);
            color: var(--light);
            resize: vertical;
            transition: border-color 0.3s ease;
        }
        
        .form-control:focus {
            outline: none;
            border-color: var(--primary);
            box-shadow: 0 0 0 2px rgba(107, 70, 193, 0.2);
        }
        
        .form-text {
            font-size: 0.85rem;
            color: var(--gray-light);
            margin-top: 0.5rem;
        }
        
        .btn-submit {
            background-color: var(--primary);
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s ease;
        }
        
        .btn-submit:hover {
            background-color: var(--primary-dark);
            transform: translateY(-2px);
            box-shadow: 0 10px 15px rgba(85, 60, 154, 0.2);
        }
        
        /* Responsive adjustments */
        @media (max-width: 768px) {
            .nav-links {
                display: none;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            .results-container {
                grid-template-columns: 1fr;
            }
            
            .rating-models {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <nav>
                <div class="logo">Brain<span>Seg</span> AI</div>
                <ul class="nav-links">
                    <li><a href="#about">About</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#ratings">Rate Models</a></li>
                    <li><a href="#faq">FAQ</a></li>
                </ul>
            </nav>
            
            <div class="hero">
                <div id="brain-container">
                    <div id="brain-model"></div>
                </div>
                <h1>From Man to Machine: Advanced Brain Segmentation AI</h1>
                <p class="tagline">A comparative study of manual vs AI-based segmentation techniques that achieves superior performance through our innovative i-UNet architecture</p>
                <a href="#ratings" class="btn">Rate Our Models</a>
            </div>
        </div>
    </header>
    
    <main>
        <section id="about" class="container">
            <h2>Our Research</h2>
            <p>Manual segmentation, while considered the gold standard in medical image processing, is known to be very time-consuming (~4 hours) and can induce inter-observer variability. Our research demonstrates how the improved U-Net architecture (i-UNet) reduces segmentation time to under 30 minutes while maintaining exceptional accuracy.</p>
            
            <div class="stats">
                <div class="stat-card">
                    <div class="stat-value">0.99</div>
                    <div class="stat-label">i-UNet Dice Score</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-value">0.01</div>
                    <div class="stat-label">i-UNet Loss</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-value">&lt;10</div>
                    <div class="stat-label">Hausdorff Distance</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-value">8×</div>
                    <div class="stat-label">Faster than Manual</div>
                </div>
            </div>
        </section>
        
        <!-- Ratings and Feedback Section -->
        <section id="ratings">
            <div class="container">
                <h2>Rate Our Models</h2>
                <p>We value your feedback on our brain segmentation models. Please rate each model and share your experiences to help us improve our research.</p>
                
                <div class="rating-models">
                    <div class="model-card">
                        <div class="model-name">Manual Segmentation</div>
                        <div class="model-description">
                            Traditional slice-by-slice manual segmentation approach using tools like ITK-SNAP & 3D Slicer. High precision but time-intensive and subject to inter-observer variability.
                        </div>
                        <div class="star-rating" data-model="manual">
                            <span class="star" data-value="1">★</span>
                            <span class="star" data-value="2">★</span>
                            <span class="star" data-value="3">★</span>
                            <span class="star" data-value="4">★</span>
                            <span class="star" data-value="5">★</span>
                        </div>
                    </div>
                    
                    <div class="model-card">
                        <div class="model-name">i-UNet Architecture</div>
                        <div class="model-description">
                            Our enhanced U-Net with attention-based mechanisms and ResNet50 backbone, offering improved feature focus and robust extraction with optimized performance.
                        </div>
                        <div class="star-rating" data-model="i-unet">
                            <span class="star" data-value="1">★</span>
                            <span class="star" data-value="2">★</span>
                            <span class="star" data-value="3">★</span>
                            <span class="star" data-value="4">★</span>
                            <span class="star" data-value="5">★</span>
                        </div>
                    </div>
                    
                    <div class="model-card">
                        <div class="model-name">3D Reconstruction</div>
                        <div class="model-description">
                            High-fidelity 3D modeling using marching cubes algorithm with precise brain fold rendering and comprehensive structural visualization.
                        </div>
                        <div class="star-rating" data-model="3d-reconstruction">
                            <span class="star" data-value="1">★</span>
                            <span class="star" data-value="2">★</span>
                            <span class="star" data-value="3">★</span>
                            <span class="star" data-value="4">★</span>
                            <span class="star" data-value="5">★</span>
                        </div>
                    </div>
                </div>
                
                <div class="comment-form">
                    <h3>Share Your Feedback</h3>
                    <form id="feedbackForm">
                        <div class="form-group">
                            <label for="name" class="form-label">Name (Optional)</label>
                            <input type="text" id="name" class="form-control" placeholder="Your name">
                        </div>
                        
                        <div class="form-group">
                            <label for="email" class="form-label">Email (Optional)</label>
                            <input type="email" id="email" class="form-control" placeholder="Your email address">
                            <div class="form-text">We'll never share your email with anyone else.</div>
                        </div>
                        
                        <div class="form-group">
                            <label for="institution" class="form-label">Institution/Organization (Optional)</label>
                            <input type="text" id="institution" class="form-control" placeholder="Your institution or organization">
                        </div>
                        
                        <div class="form-group">
                            <label for="comment" class="form-label">Comments</label>
                            <textarea id="comment" class="form-control" rows="5" placeholder="Share your thoughts, suggestions, or experiences with our models"></textarea>
                        </div>
                        
                        <button type="submit" class="btn-submit">Submit Feedback</button>
                    </form>
                </div>
            </div>
        </section>
        
        <section id="faq">
            <div class="container">
                <h2>Frequently Asked Questions</h2>
                
                <div class="faq-category">
                    <h3>1. Segmentation Model</h3>
                    <div class="faq">
                        <div class="faq-item">
                            <div class="faq-question">
                                What specific modifications were made to the U-Net to create the i-UNet, and how do these changes directly address the limitations of the standard model?
                            </div>
                            <div class="faq-answer">
                                The i-UNet builds on the classic U-Net by integrating manual attention mechanisms and a ResNet50 backbone. These enhancements help the model focus on critical regions during segmentation and improve feature extraction, which reduces noise and enhances boundary delineation. In essence, these modifications tackle issues like overfitting and loss of spatial detail that are common in standard U-Net models.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                How does the inclusion of attention mechanisms enhance the model's ability to capture global semantic information, and can you provide quantitative evidence supporting this improvement?
                            </div>
                            <div class="faq-answer">
                                The attention mechanisms allow the model to weigh significant features more effectively, ensuring that global and contextual information is prioritized. Quantitatively, this is evidenced by an improved Dice score (0.99 for i-UNet compared to 0.98 for the conventional U-Net) and a dramatic reduction in final loss (0.01 versus 0.49), highlighting a more robust and accurate segmentation process.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                Which loss function was used, and what explains such a significant disparity in convergence between i-UNet and the conventional U-Net?
                            </div>
                            <div class="faq-answer">
                                We employed a combined loss function that leverages both cross-entropy and Dice loss. This dual approach addresses class imbalance and emphasizes segmentation accuracy. The i-UNet's architectural enhancements, especially the attention mechanisms, allow it to optimize these losses more effectively, resulting in a much faster and stable convergence compared to the standard U-Net.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                While the Dice score improved from 0.98 to 0.99, is this marginal increase statistically significant, and does it translate into a clinically meaningful improvement?
                            </div>
                            <div class="faq-answer">
                                Even small improvements in the Dice score can be clinically significant, particularly when precise boundary delineation is critical. Our repeated cross-validation confirms that this improvement is consistent, ensuring that even marginal gains in segmentation accuracy can enhance clinical decision-making, especially in delicate brain imaging scenarios.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                How were the evaluation metrics (Dice score and Hausdorff distance) computed, and were techniques like cross-validation or an independent test set employed to ensure robust performance?
                            </div>
                            <div class="faq-answer">
                                Standard evaluation protocols were followed, with metrics computed across both training and validation datasets. We utilized cross-validation to confirm the model's generalizability and ensure that the performance improvements were not limited to a particular data split, thereby bolstering confidence in the i-UNet's robustness.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="faq-category">
                    <h3>2. Dataset Queries</h3>
                    <div class="faq">
                        <div class="faq-item">
                            <div class="faq-question">
                                How representative is the SynthStrip dataset of real-world clinical MRI T1 scans, and what are its limitations in terms of patient demographics and scan variability?
                            </div>
                            <div class="faq-answer">
                                The SynthStrip dataset offers high-quality, controlled images that are ideal for initial model development. However, it does have limitations: it may not fully capture the diverse range of patient demographics, scanner types, and imaging protocols found in actual clinical settings. Recognizing these limitations, we plan to validate the model further using more heterogeneous datasets.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                Have you considered validating the model on additional datasets or real clinical data to ensure that the performance improvements observed are generalizable?
                            </div>
                            <div class="faq-answer">
                                Absolutely. Future work involves validating the i-UNet on a broader spectrum of clinical datasets. Collaborations with healthcare institutions are planned to access real-world imaging data, ensuring that our model's performance improvements hold true outside controlled experimental conditions.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                What preprocessing steps were applied to the dataset, and how might these influence the segmentation outcomes compared to raw clinical scans?
                            </div>
                            <div class="faq-answer">
                                Preprocessing techniques such as normalization, skull-stripping, and bias correction were applied to improve image consistency and quality. While these steps enhance the training process, they may reduce some real-world variability. Our ongoing research will test the model on less preprocessed, raw scans to better simulate clinical conditions.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                Given the inherent variability in MRI scan quality, how does the model handle potential artifacts or noise in the input data?
                            </div>
                            <div class="faq-answer">
                                The integration of attention mechanisms and robust feature extraction layers in the i-UNet enables it to mitigate the effects of noise and artefacts. Additionally, data augmentation techniques during training simulate real-world variability, helping the model generalise better when confronted with imperfect data.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="faq-category">
                    <h3>3. Usage Queries</h3>
                    <div class="faq">
                        <div class="faq-item">
                            <div class="faq-question">
                                Considering the training was performed on a MacBook Air M1 with limited resources, what strategies are in place to scale the i-UNet for industrial or clinical applications?
                            </div>
                            <div class="faq-answer">
                                While initial development was conducted on a student-based setup, the i-UNet is inherently scalable. Plans include leveraging high-performance computing environments, cloud-based GPU clusters, and optimised inference pipelines to process larger datasets and meet real-time clinical demands without compromising on performance.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                How does the improved segmentation performance balance with computational efficiency in a production setting, and are there trade-offs that might affect deployment?
                            </div>
                            <div class="faq-answer">
                                The i-UNet is designed to strike an optimal balance between segmentation accuracy and computational efficiency. Its enhanced architecture ensures faster convergence and lower processing times, which is critical for production use. Nonetheless, continuous benchmarking in real-world scenarios will be essential to fine-tune performance and address any trade-offs.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                What plans are in place for implementing user-friendly interfaces (such as Grad-CAM visualizations) that allow clinicians to interpret the model's decision-making process and build trust in its outputs?
                            </div>
                            <div class="faq-answer">
                                We are developing intuitive visualisation tools like Grad-CAM overlays that will be integrated into the segmentation platform. These interfaces will provide clear, interpretable insights into the model's decision-making process, thereby helping clinicians understand and trust the automated outcomes.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                How will manual segmentation using other software (e.g., ITK-Snap) be integrated into the evaluation process, and how will differences in user expertise be accounted for when comparing segmentation quality?
                            </div>
                            <div class="faq-answer">
                                Our evaluation strategy includes comparative studies using manual segmentation data from ITK-Snap, performed by both novice and experienced users. This approach will help us identify and adjust for variability due to user expertise, ensuring that our model's performance is robust and consistent across different levels of manual input.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                What are the envisioned workflows for integrating the i-UNet into existing clinical systems, and how will issues such as data privacy, interoperability, and regulatory compliance be addressed?
                            </div>
                            <div class="faq-answer">
                                The integration of i-UNet into clinical workflows will be modular and compliant with healthcare standards (e.g., DICOM). We are designing secure, interoperable systems that adhere to stringent data privacy protocols, including encryption and anonymization. Regulatory compliance will be ensured through rigorous testing and adherence to established industry guidelines.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                In cases of model failure or unexpected outcomes, what contingency measures are in place to ensure patient safety and maintain clinician oversight?
                            </div>
                            <div class="faq-answer">
                                We have implemented a dual-review system where automated segmentations are always cross-checked by clinical experts. Additionally, the system is equipped with fallback protocols and real-time alerts to address any unexpected model behavior promptly, ensuring continuous clinician oversight and patient safety.
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="faq-category">
                    <h3>4. General Queries</h3>
                    <div class="faq">
                        <div class="faq-item">
                            <div class="faq-question">
                                What are the greater goals of BrainSeg AI and how can it be integrated with clinical care to improve patient outcomes?
                            </div>
                            <div class="faq-answer">
                                This software provides a basic framework for AI-based segmentation of brain MRIs, however, the greater goal is to have a singular software that aids medical image segmentation, diagnosis, prognosis, and treatment planning for all data inputs. Integration within the clinic and the user-friendliness of such an application is paramount to its success, meeting users at their level and allowing for improved clinical access in low-resource settings.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                What are some potential implications for the fields of minimally invasive surgery and soft robotics?
                            </div>
                            <div class="faq-answer">
                                Future work for BrainSeg AI includes increasing its capabilities to segment specific regions and tumours in the brain, allowing for surgical intervention planning supported by simulations on the models. Models produced by the i-Unet can be used to train medical robots for minimally invasive surgery and calibrate pressure forces at the end effectors.
                            </div>
                        </div>
                        
                        <div class="faq-item">
                            <div class="faq-question">
                                What are our reasons for creating our own network over using nnUnet and other deep learning models?
                            </div>
                            <div class="faq-answer">
                                While nnUnet has shown good performance for 3D segmentation, our model is more user friendly because it has less intensive GPU and RAM requirements. Its generalisation can also be developed further for application in brain tumour analysis or for creating phantoms for medical training.
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>
    
    <footer>
        <div class="container">
            <div class="logo" style="justify-content: center;">Brain<span>Seg</span> AI</div>
            <div class="team-members">
                A research project by Balvinder Kaur Dhillon, Róza Rebeka Somogyi, Abbas Abdirashid Mahmud Yusuf, Vivek Kesava Christopher Kindell, Kirijan Nalvelnathan
            </div>
            <div class="contact-info">
                <p>Supervised by Professor Zion Tse & Dr. S.M.Hadi Sadati</p>
                <p>© 2025 BrainSeg Research Team</p>
            </div>
        </div>
    </footer>
    <script>
        // Three.js setup for spinning brain
        document.addEventListener('DOMContentLoaded', function() {
            // Create scene
            const scene = new THREE.Scene();
            
            // Create camera
            const camera = new THREE.PerspectiveCamera(75, 1, 0.1, 1000);
            camera.position.z = 5;
            
            // Create renderer
            const renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
            renderer.setSize(250, 250);
            renderer.setClearColor(0x000000, 0);
            
            // Add renderer to DOM
            const container = document.getElementById('brain-model');
            container.appendChild(renderer.domElement);
            
            // Create brain geometry
            const brainGeometry = new THREE.SphereGeometry(1.5, 32, 32);
            
            // Create brain material with purple color and slight transparency
            const brainMaterial = new THREE.MeshPhongMaterial({
                color: 0x6B46C1,
                transparent: true,
                opacity: 0.9,
                shininess: 100
            });
            
            // Create brain mesh
            const brain = new THREE.Mesh(brainGeometry, brainMaterial);
            
            // Add complexity to brain to make it look less like a perfect sphere
            const bumps = 10;
            for (let i = 0; i < bumps; i++) {
                const bumpGeometry = new THREE.SphereGeometry(0.25 + Math.random() * 0.2, 16, 16);
                const bump = new THREE.Mesh(bumpGeometry, brainMaterial);
                
                const theta = Math.random() * Math.PI * 2;
                const phi = Math.acos(2 * Math.random() - 1);
                
                const x = 1.2 * Math.sin(phi) * Math.cos(theta);
                const y = 1.2 * Math.sin(phi) * Math.sin(theta);
                const z = 1.2 * Math.cos(phi);
                
                bump.position.set(x, y, z);
                brain.add(bump);
            }
            
            // Add brain to scene
            scene.add(brain);
            
            // Add some sulci (brain folds) with curve geometries
            const curveCount = 8;
            for (let i = 0; i < curveCount; i++) {
                // Create a random curve on the surface of the brain
                const points = [];
                const segments = 10;
                const radius = 1.6;
                
                const startAngle = Math.random() * Math.PI * 2;
                const angleWidth = (0.5 + Math.random() * 0.5) * Math.PI;
                
                for (let j = 0; j <= segments; j++) {
                    const angle = startAngle + (j / segments) * angleWidth;
                    const x = radius * Math.sin(angle) * Math.cos(j/segments * Math.PI);
                    const y = radius * Math.sin(angle) * Math.sin(j/segments * Math.PI);
                    const z = radius * Math.cos(angle);
                    
                    points.push(new THREE.Vector3(x, y, z));
                }
                
                const curve = new THREE.CatmullRomCurve3(points);
                const tubeGeometry = new THREE.TubeGeometry(curve, 20, 0.05, 8, false);
                const tubeMaterial = new THREE.MeshPhongMaterial({
                    color: 0x553C9A,
                    shininess: 50
                });
                
                const tube = new THREE.Mesh(tubeGeometry, tubeMaterial);
                scene.add(tube);
            }
            
            // Add lighting
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
            scene.add(ambientLight);
            
            const light1 = new THREE.DirectionalLight(0xffffff, 0.8);
            light1.position.set(5, 5, 5);
            scene.add(light1);
            
            const light2 = new THREE.DirectionalLight(0x6B46C1, 0.5);
            light2.position.set(-5, -5, -5);
            scene.add(light2);
            
            // Animation loop
            function animate() {
                requestAnimationFrame(animate);
                
                // Rotate brain
                brain.rotation.y += 0.005;
                brain.rotation.x += 0.002;
                brain.rotation.z += 0.001;
                
                // Render scene
                renderer.render(scene, camera);
            }
            
            animate();
            
            // Handle window resize
            window.addEventListener('resize', function() {
                // Keep aspect ratio 1:1 for the brain
                const size = Math.min(250, window.innerWidth * 0.8);
                renderer.setSize(size, size);
            });
        });
    </script>
</body>
</html>
    </script>
</body>
</html>


